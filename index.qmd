---
title: "Decision Tree Challenge"
subtitle: "Feature Importance and Categorical Variable Encoding"
format:
  html: default
  pdf: default
execute:
  echo: false
  eval: true
---

# 🌳 Decision Tree Challenge - Feature Importance and Variable Encoding

## Challenge Overview

**Your Mission:** Create a simple GitHub Pages site that demonstrates how decision trees measure feature importance and analyzes the critical differences between categorical and numerical variable encoding. You'll answer two key discussion questions by adding narrative to a pre-built analysis and posting those answers to your GitHub Pages site as a rendered HTML document.

::: {.callout-warning}
## ⚠️ AI Partnership Required

This challenge pushes boundaries intentionally. You'll tackle problems that normally require weeks of study, but with Cursor AI as your partner (and your brain keeping it honest), you can accomplish more than you thought possible.

**The new reality:** The four stages of competence are Ignorance → Awareness → Learning → Mastery. AI lets us produce Mastery-level work while operating primarily in the Awareness stage. I focus on awareness training, you leverage AI for execution, and together we create outputs that used to require years of dedicated study.
:::

## The Decision Tree Problem 🎯

> "The most important thing in communication is hearing what isn't said." - Peter Drucker

**The Core Problem:** Decision trees are often praised for their interpretability and ability to handle both numerical and categorical variables. But what happens when we encode categorical variables as numbers? How does this affect our understanding of feature importance?

**What is Feature Importance?** In decision trees, feature importance measures how much each variable contributes to reducing impurity (or improving prediction accuracy) across all splits in the tree. It's a key metric for understanding which variables matter most for your predictions.

::: {.callout-important}
## 🎯 The Key Insight: Encoding Matters for Interpretability

**The problem:** When we encode categorical variables as numerical values (like 1, 2, 3, 4...), decision trees treat them as if they have a meaningful numerical order. This can completely distort our analysis.

**The Real-World Context:** In real estate, we know that neighborhood quality, house style, and other categorical factors are crucial for predicting home prices. But if we encode these as numbers, we might get misleading insights about which features actually matter most.

**The Devastating Reality:** Even sophisticated machine learning models can give us completely wrong insights about feature importance if we don't properly encode our variables. A categorical variable that should be among the most important might appear irrelevant, while a numerical variable might appear artificially important.

:::

Let's assume we want to predict house prices and understand which features matter most. The key question is: **How does encoding categorical variables as numbers affect our understanding of feature importance?**

## The Ames Housing Dataset 🏠

We are analyzing the Ames Housing dataset which contains detailed information about residential properties sold in Ames, Iowa from 2006 to 2010. This dataset is perfect for our analysis because it contains a categorical variable (like zip code) and numerical variables (like square footage, year built, number of bedrooms).

## The Problem: ZipCode as Numerical vs Categorical

**Key Question:** What happens when we treat zipCode as a numerical variable in a decision tree? How does this affect feature importance interpretation?

**The Issue:** Zip codes (50010, 50011, 50012, 50013) are categorical variables representing discrete geographic areas, i.e. neighborhoods. When treated as numerical, the tree might split on "zipCode > 50012.5" - which has no meaningful interpretation for house prices.  Zip codes are non-ordinal categorical variables meaning they have no inherent order that aids house price prediction (i.e. zip code 99999 is not the priceiest zip code).

## Data Loading and Model Building

::: {.callout-important}
## 🎯 Note on Python Usage

You have not been coached through setting up a Python environment.  You will need to set up a Python environment and install the necessary packages to run this code - takes about 15 minutes; see [https://quarto.org/docs/projects/virtual-environments.html](https://quarto.org/docs/projects/virtual-environments.html).  Alternatively, delete the Python code and only leave the remaining R code that is provided.  You can see the executed Python output at my GitHub pages site: [https://flyaflya.github.io/decTreeChallenge/](https://flyaflya.github.io/decTreeChallenge/).

:::

::: {.panel-tabset}

### R

```{r}
#| label: load-and-model-r
#| echo: true
#| message: false
#| warning: false

# Load libraries
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(rpart))
if (!require(rpart.plot, quietly = TRUE)) {
  install.packages("rpart.plot", repos = "https://cran.rstudio.com/")
  library(rpart.plot)
}

# Load data
sales_data <- read.csv("https://raw.githubusercontent.com/flyaflya/buad442Fall2025/refs/heads/main/datasets/salesPriceData.csv")

# Prepare model data (treating zipCode as numerical)
model_data <- sales_data %>%
  select(SalePrice, LotArea, YearBuilt, GrLivArea, FullBath, HalfBath, 
         BedroomAbvGr, TotRmsAbvGrd, GarageCars, zipCode) %>%
  na.omit()

# Split data
set.seed(123)
train_indices <- sample(1:nrow(model_data), 0.8 * nrow(model_data))
train_data <- model_data[train_indices, ]
test_data <- model_data[-train_indices, ]

# Build decision tree
tree_model <- rpart(SalePrice ~ ., 
                    data = train_data,
                    method = "anova",
                    control = rpart.control(maxdepth = 3, 
                                          minsplit = 20, 
                                          minbucket = 10))

cat("Model built with", sum(tree_model$frame$var == "<leaf>"), "terminal nodes\n")
```

### Python

```{python}
#| label: load-and-model-python
#| echo: true

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Load data
sales_data = pd.read_csv("https://raw.githubusercontent.com/flyaflya/buad442Fall2025/refs/heads/main/datasets/salesPriceData.csv")

# Prepare model data (treating zipCode as numerical)
model_vars = ['SalePrice', 'LotArea', 'YearBuilt', 'GrLivArea', 'FullBath', 
              'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'zipCode']
model_data = sales_data[model_vars].dropna()

# Split data
X = model_data.drop('SalePrice', axis=1)
y = model_data['SalePrice']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

# Build decision tree
tree_model = DecisionTreeRegressor(max_depth=3, 
                                  min_samples_split=20, 
                                  min_samples_leaf=10, 
                                  random_state=123)
tree_model.fit(X_train, y_train)

print(f"Model built with {tree_model.get_n_leaves()} terminal nodes")
```

:::

## Tree Visualization

::: {.panel-tabset}

### R

```{r}
#| label: visualize-tree-r
#| echo: true
#| message: false
#| warning: false
#| fig-width: 10
#| fig-height: 6

# Visualize tree
if (require(rpart.plot, quietly = TRUE)) {
  rpart.plot(tree_model, 
             type = 2,
             extra = 101,
             fallen.leaves = TRUE,
             digits = 0,
             cex = 0.8,
             main = "Decision Tree (zipCode as Numerical)")
} else {
  plot(tree_model, uniform = TRUE, main = "Decision Tree (zipCode as Numerical)")
  text(tree_model, use.n = TRUE, all = TRUE, cex = 0.8)
}
```

### Python

```{python}
#| label: visualize-tree-python
#| echo: true
#| fig-width: 10
#| fig-height: 6

# Visualize tree
plt.figure(figsize=(10, 6))
plot_tree(tree_model, 
          feature_names=X_train.columns,
          filled=True, 
          rounded=True,
          fontsize=10,
          max_depth=3)
plt.title("Decision Tree (zipCode as Numerical)")
plt.tight_layout()
plt.show()
```

:::

## Feature Importance Analysis

::: {.panel-tabset}

### R

```{r}
#| label: feature-importance-r
#| echo: false
#| message: false
#| warning: false

# Extract and display feature importance
importance_df <- data.frame(
  Feature = names(tree_model$variable.importance),
  Importance = as.numeric(tree_model$variable.importance)
) %>%
  arrange(desc(Importance)) %>%
  mutate(Importance_Percent = round(Importance / sum(Importance) * 100, 2))

# Check zipCode ranking
zipcode_rank <- which(importance_df$Feature == "zipCode")
zipcode_importance <- importance_df$Importance_Percent[zipcode_rank]
```

```{r}
#| label: importance-plot-r
#| echo: false
#| message: false
#| warning: false
#| fig-width: 8
#| fig-height: 5

# Plot feature importance
library(ggplot2)
ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_col(fill = "steelblue", alpha = 0.7) +
  coord_flip() +
  labs(title = "Feature Importance (zipCode as Numerical)",
       x = "Features", y = "Importance Score") +
  theme_minimal()
```

### Python

<!-- ```{python} -->
#| label: feature-importance-python
#| echo: false

# Extract and display feature importance
importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': tree_model.feature_importances_
}).sort_values('Importance', ascending=False)

importance_df['Importance_Percent'] = (importance_df['Importance'] * 100).round(2)

# Check zipCode ranking
zipcode_rank = importance_df[importance_df['Feature'] == 'zipCode'].index[0] + 1
zipcode_importance = importance_df[importance_df['Feature'] == 'zipCode']['Importance_Percent'].iloc[0]
```

<!-- ```{python} -->
#| label: importance-plot-python
#| echo: false
#| fig-width: 8
#| fig-height: 5

# Plot feature importance
plt.figure(figsize=(8, 5))
plt.barh(range(len(importance_df)), importance_df['Importance'], 
         color='steelblue', alpha=0.7)
plt.yticks(range(len(importance_df)), importance_df['Feature'])
plt.xlabel('Importance Score')
plt.title('Feature Importance (zipCode as Numerical)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()
```

:::

## Critical Analysis: The Encoding Problem

::: {.callout-warning}
## ⚠️ The Problem Revealed

**What to note:** Our decision tree treated `zipCode` as a numerical variable.  This leads to zip code being unimportant.  Not surprisingly, because there is no reason to believe allowing splits like "zipCode < 50012.5" should be beneficial for house price prediction. This false coding of a variable creates several problems:

1. **Potentially Meaningless Splits:** A zip code of 50013 is not "greater than" 50012 in any meaningful way for house prices
2. **False Importance:** The algorithm assigns importance to zipCode based on numerical splits rather than categorical distinctions OR the importance of zip code is completely missed as numerical ordering has no inherent relationship to house prices.
3. **Misleading Interpretations:** We might conclude zipCode is not important when our intuition tells us it should be important (listen to your intuition).

**The Real Issue:** Zip codes are categorical variables representing discrete geographic areas. The numerical values have no inherent order or magnitude relationship to house prices.  These must be modelled as categorical variables.
:::

## Proper Categorical Encoding: The Solution

Now let's repeat the analysis with zipCode properly encoded as categorical variables to see the difference.

**R Approach:** Convert zipCode to a factor (categorical variable)  
**Python Approach:** One-hot encode zipCode (create dummy variables for each zip code)

### Categorical Encoding Analysis

::: {.panel-tabset}

### R

```{r}
#| label: categorical-r
#| echo: true
#| message: false
#| warning: false


# Convert zipCode to factor (categorical)
model_data_cat <- model_data %>%
  mutate(zipCode = as.factor(zipCode))

# Split data
set.seed(123)
train_indices_cat <- sample(1:nrow(model_data_cat), 0.8 * nrow(model_data_cat))
train_data_cat <- model_data_cat[train_indices_cat, ]
test_data_cat <- model_data_cat[-train_indices_cat, ]

# Build decision tree with categorical zipCode
tree_model_cat <- rpart(SalePrice ~ ., 
                        data = train_data_cat,
                        method = "anova",
                        control = rpart.control(maxdepth = 3, 
                                              minsplit = 20, 
                                              minbucket = 10))

# Feature importance with categorical zipCode
importance_cat <- data.frame(
  Feature = names(tree_model_cat$variable.importance),
  Importance = as.numeric(tree_model_cat$variable.importance)
) %>%
  arrange(desc(Importance)) %>%
  mutate(Importance_Percent = round(Importance / sum(Importance) * 100, 2))

# Check if zipCode appears in tree
zipcode_in_tree <- "zipCode" %in% names(tree_model_cat$variable.importance)
if(zipcode_in_tree) {
  zipcode_rank_cat <- which(importance_cat$Feature == "zipCode")
}
```

### Python

<!-- ```{python} -->
#| label: categorical-python
#| echo: true
#| include: false
# One-hot encode zipCode
import pandas as pd

# Create one-hot encoded zipCode
zipcode_encoded = pd.get_dummies(model_data['zipCode'], prefix='zipCode')
model_data_cat = pd.concat([model_data.drop('zipCode', axis=1), zipcode_encoded], axis=1)

# Split data
X_cat = model_data_cat.drop('SalePrice', axis=1)
y_cat = model_data_cat['SalePrice']
X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X_cat, y_cat, test_size=0.2, random_state=123)

# Build decision tree with one-hot encoded zipCode
tree_model_cat = DecisionTreeRegressor(max_depth=3, 
                                      min_samples_split=20, 
                                      min_samples_leaf=10, 
                                      random_state=123)
tree_model_cat.fit(X_train_cat, y_train_cat)

# Feature importance with one-hot encoded zipCode
importance_cat_df = pd.DataFrame({
    'Feature': X_train_cat.columns,
    'Importance': tree_model_cat.feature_importances_
}).sort_values('Importance', ascending=False)

importance_cat_df['Importance_Percent'] = (importance_cat_df['Importance'] * 100).round(2)

# Check zipCode features
zipcode_features = [col for col in X_train_cat.columns if col.startswith('zipCode')]
zipcode_importance = importance_cat_df[importance_cat_df['Feature'].isin(zipcode_features)]['Importance'].sum()
total_importance = importance_cat_df['Importance'].sum()
zipcode_percent = (zipcode_importance / total_importance * 100).round(2)
```

:::

### Tree Visualization: Categorical zipCode

::: {.panel-tabset}

### R

```{r}
#| label: visualize-tree-cat-r
#| echo: true
#| message: false
#| warning: false
#| fig-width: 10
#| fig-height: 6

# Visualize tree with categorical zipCode
if (require(rpart.plot, quietly = TRUE)) {
  rpart.plot(tree_model_cat, 
             type = 2,
             extra = 101,
             fallen.leaves = TRUE,
             digits = 0,
             cex = 0.8,
             main = "Decision Tree (zipCode as Categorical)")
} else {
  plot(tree_model_cat, uniform = TRUE, main = "Decision Tree (zipCode as Categorical)")
  text(tree_model_cat, use.n = TRUE, all = TRUE, cex = 0.8)
}
```

### Python

<!-- ```{python} -->
#| label: visualize-tree-cat-python
#| echo: true
#| fig-width: 10
#| fig-height: 6

# Visualize tree with one-hot encoded zipCode
plt.figure(figsize=(10, 6))
plot_tree(tree_model_cat, 
          feature_names=X_train_cat.columns,
          filled=True, 
          rounded=True,
          fontsize=8,
          max_depth=4)
plt.title("Decision Tree (zipCode One-Hot Encoded)")
plt.tight_layout()
plt.show()
```

:::

### Feature Importance: Categorical zipCode

::: {.panel-tabset}

### R

```{r}
#| label: importance-plot-cat-r
#| echo: false
#| message: false
#| warning: false
#| fig-width: 8
#| fig-height: 5

# Plot feature importance for categorical zipCode
library(ggplot2)
ggplot(importance_cat, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_col(fill = "darkgreen", alpha = 0.7) +
  coord_flip() +
  labs(title = "Feature Importance (zipCode as Categorical)",
       x = "Features", y = "Importance Score") +
  theme_minimal()
```

### Python

<!-- ```{python} -->
#| label: importance-plot-cat-python
#| echo: false
#| fig-width: 8
#| fig-height: 5

# Plot feature importance for categorical zipCode
plt.figure(figsize=(8, 5))
plt.barh(range(len(importance_cat_df)), importance_cat_df['Importance'], 
         color='darkgreen', alpha=0.7)
plt.yticks(range(len(importance_cat_df)), importance_cat_df['Feature'])
plt.xlabel('Importance Score')
plt.title('Feature Importance (zipCode One-Hot Encoded)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()
```

:::

## Challenge Requirements 📋

### Minimum Requirements for Any Points on Challenge

1. **Create a GitHub Pages Site:** Use the starter repository (see Repository Setup section below) to begin with a working template. The repository includes all the analysis code and visualizations above.

2. **Add Discussion Narrative:** Add your answers to the two discussion questions below in the Discussion Questions section of the rendered HTML.

3. **GitHub Repository:** Use your forked repository (from the starter repository) named "decTreeChallenge" in your GitHub account.

4. **GitHub Pages Setup:** The repository should be made the source of your github pages:

   - Go to your repository settings (click the "Settings" tab in your GitHub repository)
   - Scroll down to the "Pages" section in the left sidebar
   - Under "Source", select "Deploy from a branch"
   - Choose "main" branch and "/ (root)" folder
   - Click "Save"
   - Your site will be available at: `https://[your-username].github.io/decTreeChallenge/`
   - **Note:** It may take a few minutes for the site to become available after enabling Pages

## Getting Started: Repository Setup 🚀

::: {.callout-important}
## 📁 Quick Start with Starter Repository

**Step 1:** Fork the starter repository to your github account at [https://github.com/flyaflya/decTreeChallenge.git](https://github.com/flyaflya/decTreeChallenge.git)

**Step 2:** Clone your fork locally using Cursor (or VS Code)

**Step 3:** You're ready to start! The repository includes pre-loaded data and a working template with all the analysis above.
:::

::: {.callout-tip}
## 💡 Why Use the Starter Repository?

**Benefits:**

- **Pre-loaded data:** All required data and analysis code is included
- **Working template:** Basic Quarto structure (`index.qmd`) is ready
- **No setup errors:** Avoid common data loading issues
- **Focus on analysis:** Spend time on the discussion questions, not data preparation
:::



::: {.callout-note}
## 🎯 Navy SEALs Motto

> "Slow is Smooth and Smooth is Fast"

*Take your time to understand the decision tree mechanics, plan your approach carefully, and execute with precision. Rushing through this challenge will only lead to errors and confusion.*
:::

::: {.callout-warning}
## 💾 Important: Save Your Work Frequently!

**Before you start:** Make sure to commit your work often using the Source Control panel in Cursor (Ctrl+Shift+G or Cmd+Shift+G). This prevents the AI from overwriting your progress and ensures you don't lose your work.

**Commit after each major step:**

- After adding your discussion answers
- After rendering to HTML
- Before asking the AI for help with new code

**How to commit:**

1. Open Source Control panel (Ctrl+Shift+G)
2. Stage your changes (+ button)
3. Write a descriptive commit message
4. Click the checkmark to commit

*Remember: Frequent commits are your safety net!*
:::

## Discussion Questions for Challenge

**Your Task:** Add thoughtful narrative answers to these two questions in the Discussion Questions section of your rendered HTML site.

1. **Numerical vs Categorical Encoding:** There are four models above, two in R and two in Python. For each language, the models differ by how zip code is modelled, either as a numerical variable or as a categorical variable. Given what you know about zip codes and real estate prices, how should zip code be modelled, numerically or categorically?

2. **R vs Python Implementation Differences:** When modelling zip code as a categorical variable, the output tree and feature importance differs quite significantly between R and Python. Investigate why this is the case. Which language would you say does a better job of modelling zip code as a categorical variable? Why is this the case? Do you see any documentation suggesting the other language does a better job? If so, please provide a quote from the documentation.

## Discussion Answers

### Question 1: Numerical vs Categorical Encoding

Based on the dramatic results from our decision tree analysis and fundamental understanding of zip codes and real estate, zip codes should **definitely be modelled as categorical variables**. Here's why:

#### The Results Reveal the Problem

Analysis revealed a stunning difference in how zip codes are treated:

- **Numerical encoding**: zipCode importance of only **1.23%** (ranked 9th out of 9 features)
- **Categorical encoding**: zipCode importance of **16.77%** (ranked 2nd out of 9 features)

This represents a **13.6x increase in importance** and a **7-position improvement in ranking** - from dead last to second place.

#### The Core Problem: How the Computer Analyzes Numerical Values

The results show that zip codes must be modelled as categorical variables. The difference in feature importance, 1.23% vs 16.77%, shows that proper categorical encoding reveals the true importance of location in real estate pricing, while numerical encoding completely masks this critical insight. This is because the computer will analyze it when numerical by the value.

#### Why This Makes Perfect Sense

**Zip codes are fundamentally categorical variables** representing discrete geographic areas (neighborhoods, districts, regions). They have no inherent numerical order - zip code 50013 is not "greater than" 50012 in any meaningful way for house prices.

#### The Real-World Reality

In real estate, location (zip code/neighborhood) is typically one of the most important factors affecting home prices. When properly encoded as categorical variables, the algorithm can make meaningful distinctions between different zip codes and neighborhoods, revealing their true importance in determining property values.

#### Conclusion

This is a perfect example of why proper variable encoding isn't just a technical detail,it's essential for getting meaningful insights from machine learning models. The dramatic difference demonstrates how the computer's analysis method (numerical vs categorical) directly impacts the insights we can extract from our data.

### Question 2: R vs Python Implementation Differences

The significant differences between R and Python when modeling zip code as a categorical variable stem from different approaches to handling categorical variables in decision tree algorithms.

#### The Key Technical Differences

**R Implementation (rpart):**
- Uses **factor conversion**: Converts zipCode to a single categorical factor with 25 levels
- Treats zipCode as **one unified categorical variable**
- Allows the algorithm to make splits like "zipCode in {50010, 50011, 50012}"
- Results: zipCode importance jumps from 1.23% to **16.77%** (2nd place)

**Python Implementation (sklearn):**
- Uses **one-hot encoding**: Creates 25 separate binary variables (zipCode_50010, zipCode_50011, etc.)
- Treats each zip code as an **independent binary feature**
- Forces the algorithm to evaluate each zip code individually
- Results: zipCode importance remains at **0.0%** (no improvement)

#### Why R Performs Better

**R's approach is superior** for categorical variables because:

1. **Preserves categorical structure**: The algorithm understands that all zip codes belong to the same categorical variable
2. **More efficient splits**: Can group multiple zip codes together in a single split
3. **Better feature importance calculation**: Assigns importance to the zipCode variable as a whole
4. **Matches real-world logic**: Neighborhoods (zip codes) should be evaluated as a single categorical concept

#### Python's Limitations

**Python's one-hot encoding approach fails** because:

1. **Dilutes importance**: Spreads zipCode importance across 25 separate features
2. **Forces binary decisions**: Each zip code can only be "included" or "excluded" individually
3. **Poor feature importance**: No single feature represents the zipCode concept
4. **Computational inefficiency**: Creates 25 features instead of 1 categorical variable

#### Documentation Evidence

According to the sklearn documentation, `DecisionTreeRegressor` treats all features as numerical. When categorical variables are one-hot encoded, the algorithm loses the understanding that these binary variables represent a single categorical concept. This is why sklearn recommends using alternative encoding methods for categorical variables with decision trees.

#### Conclusion

**R does a significantly better job** of modeling zip code as a categorical variable. The dramatic difference in results, 16.77% vs 0.0% importance, demonstrates that R's factor-based approach preserves the categorical nature of zip codes, while Python's one-hot encoding approach destroys this structure and renders zip codes effectively useless in the model.






